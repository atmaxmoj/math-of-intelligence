# 核心想法：语法-语义-优化的统一理论

日期：2026-01-30

## 问题

为什么 CNN 对图像好？为什么 Counter 对计数好？

不是问"能不能算"（图灵机都能算），而是问"为什么学得好"。

## 错误的回答

- "表达力刚好够"（紧致性假说）→ 实验反驳了
- "平移等变"→ 只说了 CNN 是什么，没说为什么对图像好
- "描述长度短"→ 太静态，没考虑学习过程

## 正确的方向

> **最优语法 = 使得损失函数在函数空间上最容易下降的参数化方式**

### 形式化

设目标函数为 f*，模型为 f_θ，损失为 L(θ) = d(f_θ, f*)

不同的"语法"（模型架构）= 不同的参数化方式 θ → f_θ

同一个 f* 在不同参数化下有不同的损失景观。

**最优语法**：使得 L(θ) 的景观"最好"
- 曲率均匀（条件数小）
- 局部极小少
- 梯度指向全局最优
- Fisher 信息矩阵结构好

### 为什么 CNN 对图像好

不只是"能表达平移不变函数"，而是**参数化方式让优化景观简单**：
- 全连接：参数空间大，很多等价点（冗余）
- CNN：参数空间紧凑，每个点唯一对应一个函数

### 更深的问题（Holy Grail）

> 给定语义 S，什么语法 G 使得学习 S 的优化景观最简单？

这是语法和语义之间的**最优配对**问题。

## 数学工具

1. **信息几何**：Fisher 信息矩阵定义参数空间的黎曼度量
2. **自然梯度**：在 Fisher 度量下的最速下降
3. **范畴论**：语法-语义的伴随关系
4. **Coalgebra**：同时捕捉行为（语义）和状态转移结构（语法）

## 待探索

- [ ] 信息几何如何刻画"好的参数化"？
- [ ] 能否证明：当模型结构匹配数据生成结构时，Fisher 信息矩阵有好的性质？
- [ ] Coalgebra 框架下如何表达这个对应？
- [ ] 这和 Solomonoff 归纳 / MDL 是什么关系？

## 相关文献待查

- 信息几何与深度学习
- 损失景观分析
- 自然梯度与架构设计
- 范畴论视角的机器学习

---

# 进一步猜想：从对称性反推最优架构

## 核心猜想

> 设目标函数类 F 有对称群 G。
> 则 Fisher 条件数最优的参数化是：参数空间 / G（商掉对称性）

即：**最优语法 = 语义对称性的商**

## 证明骨架（正向，容易）

```
1. 设目标函数 f* 是 G-不变的

2. 全连接参数化：θ ∈ R^N
   存在 G 作用使得 f_θ = f_{g·θ}
   → 参数空间有等价类
   → Jacobian ∂f/∂θ 有非平凡核（维度 = |G|-1）
   → Fisher 矩阵继承这个核
   → 条件数 → ∞

3. 商空间参数化：θ ∈ R^N / G
   等价类已 quotient 掉
   → Jacobian 满秩
   → Fisher 条件数 O(1)
```

## 反向（难，需要证）

给定 G，证明商空间参数化是**唯一**使 Fisher 条件数最优的。

## 推论（如果猜想成立）

| 任务 | 对称群 G | 最优架构 |
|------|----------|----------|
| 图像分类 | 平移群 | CNN |
| 图像 + 旋转 | SE(2) | 旋转等变网络 |
| 集合输入 | 置换群 S_n | DeepSets / Transformer |
| 序列 | 时间平移(?) | RNN(?) |
| 图 | 顶点置换 | GNN |

## 难点和待解决问题

1. "最优"的精确定义：Fisher 条件数？还是别的？
2. G 在参数空间上的作用怎么严格定义？
3. 非线性激活函数怎么处理？
4. 商空间的参数化是否唯一？（可能只唯一到同构）
5. 和 Cohen-Welling 等变网络理论的关系

## 与现有工作的区别

Cohen-Welling (2016): "如果你想要 G-等变，用 G-卷积"（充分性）

本猜想: "G-卷积（商空间参数化）是唯一 Fisher 最优的"（必要性 + 最优性）

## 状态

纯直觉，需要严格化。可能是本科生级别的结果，也可能有坑。

---

# 根本性问题：语义信息是 Well-Founded 的吗？

## 问题来源

如果"智能 = 压缩"（Hutter/DeepMind），那么：

```
compress(X) = 对 X 的理解
compress(compress(X)) = 对"理解X"的理解
...
```

这形成一条"下降链"。如果语义大小是自然数，链必须终止（ℕ 是 well-founded）。

**但语义大小凭什么是自然数？**

## 隐藏的假设

```
Shannon 信息 = bits = ℕ ✓ (well-defined)
Kolmogorov 复杂度 = 程序长度 = ℕ ✓ (well-defined, 虽然不可计算)
语义内容 = ???
```

我们假设了：
```
语义大小 : Semantics → ℕ
```

这个映射存在吗？唯一吗？

## 可能的情况

### 1. 语义是偏序，不是全序

```
"理解加法" vs "理解素数分布"

哪个语义更"大"？可能根本不可比。
```

如果语义只有偏序结构，"下降链"的概念就不全局适用。

### 2. 语义用序数度量，但不一定是 ω

如果语义大小是比 ω 更大的序数，well-foundedness 仍然成立，但结构更复杂。

### 3. 语义根本不是序数

语义可能是：
- 一个范畴（对象 + 态射，不只是大小比较）
- 一个格（有 meet 和 join）
- 一个拓扑空间（有连续性概念）
- 某种更奇异的结构

## 对框架的影响

如果语义不是 well-founded：

```
MetaLearn 的迭代可能不收敛
L = MetaLearn(L) 可能不存在
或存在多个不可比的"不动点"
整个"学习的学习"框架需要重新审视
```

## 更根本的问题

也许不应该问"语义的大小"，而应该问：

1. **语义形成什么结构？**（范畴？格？拓扑空间？）
2. **"压缩"在这个结构上是什么操作？**（函子？态射？）
3. **不动点在这个结构中意味着什么？**（终对象？极限？）

## 可能的出路

### 方案 A：证明语义确实有 ℕ 值的度量

需要定义一个具体的语义复杂度函数，证明它是 well-defined 的。

### 方案 B：用序数代替自然数

允许超穷序数，但仍保持 well-foundedness。

### 方案 C：放弃全局度量，用局部/相对概念

不问"X 有多大"，而问"X 相对于 Y 的复杂度"。

### 方案 D：用范畴论重新表述

不用"大小"，用态射、函子、自然变换来刻画压缩和学习。

## 状态

这是整个框架的一个**根本性未解决问题**。

在这个问题解决之前，所有关于 MetaLearn 不动点的讨论都建立在未经证明的假设上。
