\chapter{Experiments: What Works and What Doesn't}

\begin{goals}
\begin{itemize}
    \item See concrete examples where architecture matters
    \item Observe patterns: symmetry seems to help
    \item Gather empirical evidence before building theory
\end{itemize}
\end{goals}

\section{Experiment 1: Counting}

\textbf{Task}: Given a sequence, count occurrences of symbol $a$.

\textbf{Setup A}: Fully connected network
\begin{itemize}
    \item Input: one-hot sequence, flattened
    \item Output: count (regression)
    \item Parameters: $O(n^2)$ where $n$ = sequence length
\end{itemize}

\textbf{Setup B}: Recurrent network (counter)
\begin{itemize}
    \item State: running count
    \item Update: if $a$, increment; else, keep
    \item Parameters: $O(1)$
\end{itemize}

\textbf{Result}: Setup B learns instantly. Setup A struggles, needs much more data.

\section{Experiment 2: Image Classification}

\textbf{Task}: Classify images (e.g., MNIST digits).

\textbf{Setup A}: Fully connected
\begin{itemize}
    \item Flatten image to vector
    \item Dense layers
    \item Parameters: $O(n^2)$ for $n$ pixels
\end{itemize}

\textbf{Setup B}: Convolutional
\begin{itemize}
    \item Local kernels, shared across positions
    \item Parameters: $O(k^2)$ for kernel size $k$
\end{itemize}

\textbf{Result}: CNN learns much faster, generalizes better, needs less data.

\section{Experiment 3: Set Functions}

\textbf{Task}: Given a set of numbers, compute some function (e.g., sum, max).

\textbf{Setup A}: Feed as ordered sequence to MLP

\textbf{Setup B}: Permutation-invariant architecture (DeepSets)
\[
f(\{x_1, \ldots, x_n\}) = \rho\left( \sum_i \phi(x_i) \right)
\]

\textbf{Result}: Setup A fails badly (learns spurious order dependence). Setup B works.

\section{The Pattern}

\begin{center}
\begin{tabular}{lll}
\textbf{Task} & \textbf{Task Symmetry} & \textbf{Best Architecture} \\
\hline
Counting & Time-shift invariant & RNN / Counter \\
Image & Translation invariant & CNN \\
Set function & Permutation invariant & DeepSets \\
Graph function & Node permutation & GNN \\
\end{tabular}
\end{center}

\begin{keyinsight}
When the architecture respects the task's symmetry, learning is easy.

When it doesn't, the network wastes capacity learning what it should get for free.
\end{keyinsight}

\section{Questions}

\begin{enumerate}
    \item \textbf{Why} does matching symmetry help?
    \item Can we \textbf{quantify} how much it helps?
    \item Given a task, can we \textbf{derive} the right architecture?
\end{enumerate}

The next chapters develop the theory to answer these.

\section{Numerical Verification: Distance Matching}

We ran experiments to test the \textbf{distance matching hypothesis}: good parameterization means parameter distance $\approx$ function distance.

\subsection{Methodology}

For each architecture:
\begin{enumerate}
    \item Sample many random parameter pairs $(\theta_1, \theta_2)$
    \item Compute parameter distance $d_\Theta(\theta_1, \theta_2) = \|\theta_1 - \theta_2\|$
    \item Compute function distance $d_{\mathcal{F}}(f_{\theta_1}, f_{\theta_2}) = \|f_{\theta_1}(x) - f_{\theta_2}(x)\|$
    \item Measure correlation and coefficient of variation of the ratio
\end{enumerate}

High correlation + low CV = good distance matching.

\subsection{Result 1: Redundancy Destroys Matching}

\begin{center}
\begin{tabular}{lcc}
\textbf{Model} & \textbf{Correlation} & \textbf{Rating} \\
\hline
Linear (direct) & 0.96 & Excellent \\
Linear $(a-b)$ parameterization & 0.48 & Poor \\
Linear $(a-b+c-d)$ & 0.26 & Terrible \\
Matrix $A \cdot B$ factorization & 0.26 & Terrible \\
\end{tabular}
\end{center}

\begin{keyinsight}
Redundant parameterization destroys distance matching. The more redundancy, the worse.
\end{keyinsight}

\subsection{Result 2: Depth Destroys Matching (Even Without Nonlinearity!)}

\begin{center}
\begin{tabular}{lcc}
\textbf{Model} & \textbf{Correlation} & \textbf{Rating} \\
\hline
1-layer linear & 0.97 & Excellent \\
2-layer linear ($W_2 W_1 x$) & 0.26 & Terrible \\
3-layer linear & 0.31 & Terrible \\
4-layer linear & 0.28 & Terrible \\
\end{tabular}
\end{center}

\begin{keyinsight}
Deep linear networks have the \emph{same function space} as shallow ones (all linear functions), yet distance matching collapses!

Why? Because $W_2 \cdot W_1 = W_2' \cdot W_1'$ has infinitely many solutions. Depth introduces \emph{implicit redundancy}.
\end{keyinsight}

\subsection{Result 3: Sparsity Helps}

\begin{center}
\begin{tabular}{lcc}
\textbf{Model} & \textbf{Correlation} & \textbf{Rating} \\
\hline
Dense linear (20 params) & 0.91 & Excellent \\
Sparse linear (5 params, fixed positions) & 0.98 & Excellent \\
Soft-sparse (sigmoid mask) & 0.48 & Poor \\
\end{tabular}
\end{center}

Fixed sparsity reduces parameters and improves matching. ``Soft'' sparsity with learnable masks introduces redundancy.

\subsection{Result 4: Matrix Factorization Always Hurts}

\begin{center}
\begin{tabular}{lcc}
\textbf{Model} & \textbf{Correlation} & \textbf{Rating} \\
\hline
Direct linear & 0.96 & Excellent \\
Low-rank $k=2$ & 0.31 & Terrible \\
Low-rank $k=5$ & 0.29 & Terrible \\
SVD parameterization & 0.32 & Terrible \\
\end{tabular}
\end{center}

Any factorization introduces redundancy.

\subsection{Result 5: Nonlinearity Is Not The Main Culprit}

\begin{center}
\begin{tabular}{lcc}
\textbf{2-layer MLP with:} & \textbf{Correlation} & \textbf{Rating} \\
\hline
Identity (linear) & 0.13 & Terrible \\
ReLU & 0.20 & Terrible \\
Tanh & 0.19 & Terrible \\
Sigmoid & 0.17 & Terrible \\
\end{tabular}
\end{center}

Even the ``linear'' 2-layer network (identity activation) has terrible matching!

\begin{keyinsight}
The problem is \textbf{depth}, not nonlinearity. The 2-layer structure $W_2 \cdot W_1$ already has redundancy, regardless of activation function.
\end{keyinsight}

\subsection{Summary: Hierarchy of Destruction}

Factors that destroy distance matching (in order of importance):

\begin{enumerate}
    \item \textbf{Explicit redundancy}: $a-b$ parameterization, matrix factorization
    \item \textbf{Depth}: even linear depth creates implicit redundancy
    \item \textbf{Complex structure}: attention (QKV), soft masks
\end{enumerate}

\subsection{The Puzzle}

If depth destroys distance matching, why do deep networks work at all?

Possible answers:
\begin{itemize}
    \item They barely work (training is indeed hard)
    \item Skip connections help (ResNet, see next section)
    \item Expressiveness gains outweigh optimization costs
    \item Special initialization/normalization compensate
\end{itemize}

\section{Open Question: Why Does ResNet Work?}

Our experiments show:
\begin{itemize}
    \item Plain deep networks: poor matching, degrades with depth
    \item ResNet: CV (variability) stays more stable with depth
\end{itemize}

Skip connections make the network ``closer to identity,'' which may preserve some distance matching properties. This needs more investigation.
